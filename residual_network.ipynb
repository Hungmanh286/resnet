{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "dataset = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "nums_classes = len(dataset.classes)\n",
    "print(f\"Number of classes: {nums_classes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNet Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 56, 56])\n"
     ]
    }
   ],
   "source": [
    "class Bottleneck(nn.Module):\n",
    "\n",
    "    def __init__(self,in_channels,intermediate_channels,expansion,is_Bottleneck,stride):\n",
    "        \n",
    "        \"\"\"\n",
    "        Creates a Bottleneck with conv 1x1->3x3->1x1 layers.\n",
    "        \n",
    "        Note:\n",
    "          1. Addition of feature maps occur at just before the final ReLU with the input feature maps\n",
    "          2. if input size is different from output, select projected mapping or else identity mapping.\n",
    "          3. if is_Bottleneck=False (3x3->3x3) are used else (1x1->3x3->1x1). Bottleneck is required for resnet-50/101/152\n",
    "        Args:\n",
    "            in_channels (int) : input channels to the Bottleneck\n",
    "            intermediate_channels (int) : number of channels to 3x3 conv \n",
    "            expansion (int) : factor by which the input #channels are increased\n",
    "            stride (int) : stride applied in the 3x3 conv. 2 for first Bottleneck of the block and 1 for remaining\n",
    "\n",
    "        Attributes:\n",
    "            Layer consisting of conv->batchnorm->relu\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        super(Bottleneck,self).__init__()\n",
    "\n",
    "        self.expansion = expansion\n",
    "        self.in_channels = in_channels\n",
    "        self.intermediate_channels = intermediate_channels\n",
    "        self.is_Bottleneck = is_Bottleneck\n",
    "        \n",
    "        # i.e. if dim(x) == dim(F) => Identity function\n",
    "        if self.in_channels==self.intermediate_channels*self.expansion:\n",
    "            self.identity = True\n",
    "        else:\n",
    "            self.identity = False\n",
    "            projection_layer = []\n",
    "            projection_layer.append(nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels*self.expansion, kernel_size=1, stride=stride, padding=0, bias=False ))\n",
    "            projection_layer.append(nn.BatchNorm2d(self.intermediate_channels*self.expansion))\n",
    "            # Only conv->BN and no ReLU\n",
    "            # projection_layer.append(nn.ReLU())\n",
    "            self.projection = nn.Sequential(*projection_layer)\n",
    "\n",
    "        # commonly used relu\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # is_Bottleneck = True for all ResNet 50+\n",
    "        if self.is_Bottleneck:\n",
    "            # bottleneck\n",
    "            # 1x1\n",
    "            self.conv1_1x1 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels, kernel_size=1, stride=1, padding=0, bias=False )\n",
    "            self.batchnorm1 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "            \n",
    "            # 3x3\n",
    "            self.conv2_3x3 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False )\n",
    "            self.batchnorm2 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "            \n",
    "            # 1x1\n",
    "            self.conv3_1x1 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels*self.expansion, kernel_size=1, stride=1, padding=0, bias=False )\n",
    "            self.batchnorm3 = nn.BatchNorm2d( self.intermediate_channels*self.expansion )\n",
    "        \n",
    "        else:\n",
    "            # basicblock\n",
    "            # 3x3\n",
    "            self.conv1_3x3 = nn.Conv2d(in_channels=self.in_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=stride, padding=1, bias=False )\n",
    "            self.batchnorm1 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "            \n",
    "            # 3x3\n",
    "            self.conv2_3x3 = nn.Conv2d(in_channels=self.intermediate_channels, out_channels=self.intermediate_channels, kernel_size=3, stride=1, padding=1, bias=False )\n",
    "            self.batchnorm2 = nn.BatchNorm2d(self.intermediate_channels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # input stored to be added before the final relu\n",
    "        in_x = x\n",
    "\n",
    "        if self.is_Bottleneck:\n",
    "            # conv1x1->BN->relu\n",
    "            x = self.relu(self.batchnorm1(self.conv1_1x1(x)))\n",
    "            \n",
    "            # conv3x3->BN->relu\n",
    "            x = self.relu(self.batchnorm2(self.conv2_3x3(x)))\n",
    "            \n",
    "            # conv1x1->BN\n",
    "            x = self.batchnorm3(self.conv3_1x1(x))\n",
    "        \n",
    "        else:\n",
    "            # conv3x3->BN->relu\n",
    "            x = self.relu(self.batchnorm1(self.conv1_3x3(x)))\n",
    "\n",
    "            # conv3x3->BN\n",
    "            x = self.batchnorm2(self.conv2_3x3(x))\n",
    "\n",
    "\n",
    "        # identity or projected mapping\n",
    "        if self.identity:\n",
    "            x += in_x\n",
    "        else:\n",
    "            x += self.projection(in_x)\n",
    "\n",
    "        # final relu\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "# Bottleneck(64*4,64,4,stride=1)\n",
    "\n",
    "def test_Bottleneck():\n",
    "    x = torch.randn(1,64,112,112)\n",
    "    model = Bottleneck(64,64,4,True,2)\n",
    "    print(model(x).shape)\n",
    "    del model\n",
    "\n",
    "test_Bottleneck()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, resnet_variant,in_channels,num_classes):\n",
    "        \"\"\"\n",
    "        Creates the ResNet architecture based on the provided variant. 18/34/50/101 etc.\n",
    "        Based on the input parameters, define the channels list, repeatition list along with expansion factor(4) and stride(3/1)\n",
    "        using _make_blocks method, create a sequence of multiple Bottlenecks\n",
    "        Average Pool at the end before the FC layer \n",
    "\n",
    "        Args:\n",
    "            resnet_variant (list) : eg. [[64,128,256,512],[3,4,6,3],4,True]\n",
    "            in_channels (int) : image channels (3)\n",
    "            num_classes (int) : output #classes \n",
    "\n",
    "        Attributes:\n",
    "            Layer consisting of conv->batchnorm->relu\n",
    "\n",
    "        \"\"\"\n",
    "        super(ResNet,self).__init__()\n",
    "        self.channels_list = resnet_variant[0]\n",
    "        self.repeatition_list = resnet_variant[1]\n",
    "        self.expansion = resnet_variant[2]\n",
    "        self.is_Bottleneck = resnet_variant[3]\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=in_channels, out_channels=64, kernel_size=7, stride=2, padding=3, bias=False )\n",
    "        self.batchnorm1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3,stride=2,padding=1)\n",
    "\n",
    "        self.block1 = self._make_blocks( 64 , self.channels_list[0], self.repeatition_list[0], self.expansion, self.is_Bottleneck, stride=1 )\n",
    "        self.block2 = self._make_blocks( self.channels_list[0]*self.expansion , self.channels_list[1], self.repeatition_list[1], self.expansion, self.is_Bottleneck, stride=2 )\n",
    "        self.block3 = self._make_blocks( self.channels_list[1]*self.expansion , self.channels_list[2], self.repeatition_list[2], self.expansion, self.is_Bottleneck, stride=2 )\n",
    "        self.block4 = self._make_blocks( self.channels_list[2]*self.expansion , self.channels_list[3], self.repeatition_list[3], self.expansion, self.is_Bottleneck, stride=2 )\n",
    "\n",
    "        self.average_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear( self.channels_list[3]*self.expansion , num_classes)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.relu(self.batchnorm1(self.conv1(x)))\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.block1(x)\n",
    "        \n",
    "        x = self.block2(x)\n",
    "        \n",
    "        x = self.block3(x)\n",
    "        \n",
    "        x = self.block4(x)\n",
    "        \n",
    "        x = self.average_pool(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "    def _make_blocks(self,in_channels,intermediate_channels,num_repeat, expansion, is_Bottleneck, stride):\n",
    "        \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels : #channels of the Bottleneck input\n",
    "            intermediate_channels : #channels of the 3x3 in the Bottleneck\n",
    "            num_repeat : #Bottlenecks in the block\n",
    "            expansion : factor by which intermediate_channels are multiplied to create the output channels\n",
    "            is_Bottleneck : status if Bottleneck in required\n",
    "            stride : stride to be used in the first Bottleneck conv 3x3\n",
    "\n",
    "        Attributes:\n",
    "            Sequence of Bottleneck layers\n",
    "\n",
    "        \"\"\"\n",
    "        layers = [] \n",
    "\n",
    "        layers.append(Bottleneck(in_channels,intermediate_channels,expansion,is_Bottleneck,stride=stride))\n",
    "        for num in range(1,num_repeat):\n",
    "            layers.append(Bottleneck(intermediate_channels*expansion,intermediate_channels,expansion,is_Bottleneck,stride=1))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "model_parameters = {}\n",
    "model_parameters['resnet50'] = ([64,128,256,512],[3,4,6,3],4,True)\n",
    "\n",
    "def test_ResNet(params):\n",
    "    model = ResNet( params , in_channels=1, num_classes=10)\n",
    "    x = torch.randn(1,1,224,224)\n",
    "    output = model(x)\n",
    "    print(output.shape)\n",
    "    return model\n",
    "\n",
    "architecture = 'resnet50'\n",
    "model = test_ResNet(model_parameters[architecture])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataset):\n",
    "    model.train()\n",
    "    train_loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(5):\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            input = data.to(\"cuda\")\n",
    "            target = target.to(\"cuda\")\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            print(f\"Epoch {epoch}, Loss {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss 2.5712122917175293\n",
      "Epoch 0, Loss 3.1214475631713867\n",
      "Epoch 0, Loss 3.0828795433044434\n",
      "Epoch 0, Loss 2.3920435905456543\n",
      "Epoch 0, Loss 2.130528688430786\n",
      "Epoch 0, Loss 2.948014497756958\n",
      "Epoch 0, Loss 2.452024221420288\n",
      "Epoch 0, Loss 2.4424033164978027\n",
      "Epoch 0, Loss 1.4835985898971558\n",
      "Epoch 0, Loss 1.7498005628585815\n",
      "Epoch 0, Loss 2.264801263809204\n",
      "Epoch 0, Loss 0.8600941300392151\n",
      "Epoch 0, Loss 1.3361446857452393\n",
      "Epoch 0, Loss 1.4393831491470337\n",
      "Epoch 0, Loss 1.214158535003662\n",
      "Epoch 0, Loss 1.1327356100082397\n",
      "Epoch 0, Loss 0.761488139629364\n",
      "Epoch 0, Loss 0.885386049747467\n",
      "Epoch 0, Loss 1.0926777124404907\n",
      "Epoch 0, Loss 0.766018807888031\n",
      "Epoch 0, Loss 0.9756751656532288\n",
      "Epoch 0, Loss 0.5201848149299622\n",
      "Epoch 0, Loss 0.9786728620529175\n",
      "Epoch 0, Loss 0.7010740637779236\n",
      "Epoch 0, Loss 0.7358401417732239\n",
      "Epoch 0, Loss 0.5087436437606812\n",
      "Epoch 0, Loss 0.6221832633018494\n",
      "Epoch 0, Loss 0.45084699988365173\n",
      "Epoch 0, Loss 0.5215899348258972\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m model.to(device)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(model, dataset)\u001b[39m\n\u001b[32m     10\u001b[39m output = model(data)\n\u001b[32m     11\u001b[39m loss = criterion(output, target)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m optimizer.step()\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss.item()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/resnet/.venv/lib/python3.11/site-packages/torch/_tensor.py:626\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    618\u001b[39m         Tensor.backward,\n\u001b[32m    619\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    624\u001b[39m         inputs=inputs,\n\u001b[32m    625\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m626\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    627\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    628\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/resnet/.venv/lib/python3.11/site-packages/torch/autograd/__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/projects/resnet/.venv/lib/python3.11/site-packages/torch/autograd/graph.py:823\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    821\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    822\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    824\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    825\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    826\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    827\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "train(model, dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
